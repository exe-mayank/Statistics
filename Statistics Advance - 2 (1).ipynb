{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "a73daaa3-5d62-4d99-8501-feb7e33050ba",
      "cell_type": "markdown",
      "source": "Question1: Define the z-statistic and explain its relationship to the standard normal distribution. How is the z-statistic used in hypothesis testing?",
      "metadata": {}
    },
    {
      "id": "6774c731-2240-4670-96a5-2f4b988491c4",
      "cell_type": "markdown",
      "source": "The z-statistic, also known as the z-score, is a statistical measure that describes a data point's position in relation to the mean of a distribution, \nexpressed in terms of standard deviations. Specifically, it measures how many standard deviations a data point is away from the mean of the distribution.\n\nRelationship to the Standard Normal Distribution:\nThe z-statistic follows a standard normal distribution (also known as the z-distribution), which is a normal distribution with a mean of 0 and a standard deviation of 1. This relationship is critical because the standard normal distribution can be used to calculate the probability of a value occurring within a given range of z-scores.\n\nA z-score of 0 means the value is exactly at the mean.\n\nPositive z-scores indicate values above the mean.\n\nNegative z-scores indicate values below the mean.\n\nBy transforming data into z-scores, we can make inferences about how likely or unlikely an observation is within a normal distribution. The z-score provides a way to standardize data, allowing for comparison across different distributions.\n\nUse of the Z-Statistic in Hypothesis Testing:\nIn hypothesis testing, the z-statistic is commonly used in z-tests to determine whether a sample mean significantly differs from a hypothesized population mean. Here's how it's used:\n\nFormulate Hypotheses:\n\nNull Hypothesis (H‚ÇÄ): The assumption that there is no effect or difference \n\nAlternative Hypothesis (H‚ÇÅ): The assumption that there is a significant effect or difference \n\nCalculate the Z-Statistic: Using the formula for the z-statistic, calculate the z-value for the observed data.\n\nDetermine the Critical Value or P-Value:\n\nThe critical value corresponds to the z-value that defines the rejection region for the test (usually at a chosen significance level, such as 0.05).\n\nAlternatively, the p-value is computed, which is the probability of obtaining a z-statistic as extreme as the observed one, assuming the null hypothesis is true.\n\nDecision Rule:\n\nIf the z-statistic falls within the critical region (i.e., the extreme tail of the normal distribution), or if the p-value is less than the significance level (e.g., ùëù < 0.05), we reject the null hypothesis in favor of the alternative hypothesis.\n\nIf the z-statistic does not fall in the rejection region (i.e., it is close to 0 or within the middle of the distribution), or if the p-value is greater than the significance level, we fail to reject the null hypothesis.",
      "metadata": {}
    },
    {
      "id": "337039a8-4580-44ba-85e1-e7269382bf24",
      "cell_type": "markdown",
      "source": "Question2 : What is a p-value, and how is it used in hypothesis testing? What does it mean if the p-value is\nvery small (e.g., 0.01)?",
      "metadata": {}
    },
    {
      "id": "6297bac5-5a5e-4944-8564-c34fd5aa25a8",
      "cell_type": "markdown",
      "source": "A p-value (short for \"probability value\") is a measure used in statistical hypothesis testing to help determine the strength of the evidence against the null hypothesis. Specifically, the p-value quantifies the probability of obtaining test results at least as extreme as the ones observed, assuming that the null hypothesis is true.\n\nIn simpler terms, the p-value helps answer the question: How likely is it to observe the data (or something more extreme) if the null hypothesis is correct?\n\nHow is the P-Value Used in Hypothesis Testing?\nThe p-value is used in hypothesis testing to decide whether to reject or fail to reject the null hypothesis (ùêª0):\n\nFormulate Hypotheses:\n\nNull Hypothesis (H‚ÇÄ): The assumption that there is no effect, difference, or relationship (e.g., ùúá=ùúá0).\n\nAlternative Hypothesis (H‚ÇÅ): The assumption that there is a significant effect, difference, or relationship (e.g., ùúá‚â†ùúá0).\n\nCalculate the Test Statistic: Compute the test statistic (e.g., z-statistic, t-statistic) based on your sample data.\n\nFind the P-Value: The p-value is calculated from the test statistic. This is the probability of observing a test statistic at least as extreme as the one you computed, given that the null hypothesis is true.\n\nDecision Rule:\n\nIf the p-value is less than or equal to the chosen significance level (ùõº), you reject the null hypothesis. This indicates that the observed data is sufficiently unlikely under the null hypothesis, providing evidence for the alternative hypothesis.\n\nIf the p-value is greater than the significance level (ùõº), you fail to reject the null hypothesis. This suggests that the observed data is not unusual enough to cast doubt on the null hypothesis.\n\nThe significance level (ùõº) is a threshold you set before testing, often 0.05 (5%) or 0.01 (1%).\n\nWhat Does It Mean if the P-Value is Very Small (e.g., 0.01)?\nIf the p-value is very small (for example, 0.01), it means that there is a low probability of observing the data (or something more extreme) if the null hypothesis were true. In this case, a small p-value suggests strong evidence against the null hypothesis.\n\nFor instance, a p-value of 0.01 means that there is only a 1% chance of observing the sample data (or more extreme results) if the null hypothesis is true. This is considered strong evidence to reject the null hypothesis in favor of the alternative hypothesis, assuming the significance level (ùõº) is set at 0.05.",
      "metadata": {}
    },
    {
      "id": "b1a2df09-951d-4a92-b9cc-4d307e2c1de3",
      "cell_type": "markdown",
      "source": "Question3: Compare and contrast the binomial and Bernoulli distributions. ",
      "metadata": {}
    },
    {
      "id": "11d41a34-ae6f-439a-b8d8-29374e23b461",
      "cell_type": "markdown",
      "source": "The binomial and Bernoulli distributions are both discrete probability distributions, but they are used to model different types of events and have distinct characteristics. Let's compare and contrast the two distributions:\n\nBernoulli Distribution:\n\nThe Bernoulli distribution models a single trial or experiment where there are only two possible outcomes: \"success\" (usually denoted as 1) or \"failure\" (denoted as 0).\n\nIt describes the probability of success in a single trial.\n\nThe distribution has only one parameter: ùëù,  which is the probability of success.\n\nBinomial Distribution:\n\nThe binomial distribution is an extension of the Bernoulli distribution to multiple independent trials. It models the number of successes in n independent Bernoulli trials, where each trial has the same probability p of success.\n\nThe binomial distribution describes the total number of successes across a series of n trials.\n\n2. Number of Trials\nBernoulli Distribution:\n\nOnly models a single trial (i.e.,n=1).\n\nIt's a special case of the binomial distribution with n=1.\n\nBinomial Distribution:\n\nModels multiple trials (i.e.,n>1).\n\nIt counts the number of successes across a fixed number of trials.\n\n3. Parameter(s)\nBernoulli Distribution:\n\nIt has one parameter: p, which is the probability of success.\n\nBinomial Distribution:\n\nIt has two parameters: n, the number of trials, and p, the probability of success in each trial.\n\n4. Outcome Range\nBernoulli Distribution:\n\nThe possible outcomes are binary: either 0 (failure) or 1 (success).\n\nBinomial Distribution:\n\nThe possible outcomes are the number of successes in n trials. So, the outcome can range from 0 (no successes) to n (all successes), i.e., k‚àà{0,1,2,‚Ä¶,n}.\n\n\n5. Mean and Variance\nBernoulli Distribution:\n\nThe mean of a Bernoulli random variable X  Œº=p\nThe variance of X =p(1‚àíp)\nBinomial Distribution:\n\nThe mean of a binomial random variable X Œº=np\nThe variance of X =np(1‚àíp)\n\n6. Relationship\nBernoulli Distribution is a special case of the binomial distribution where n=1.\n\nIn fact, if you have a Bernoulli trial, the number of successes can only be 0 or 1, and this fits the definition of a binomial distribution with n=1\n\n\n7. Applications\nBernoulli Distribution:\n\nUsed to model situations where there is only one trial or experiment with two possible outcomes (e.g., flipping a coin, success/failure in a single trial, or pass/fail in an exam).\n\nBinomial Distribution:\n\nUsed to model situations with multiple trials, such as:\n\nThe number of heads in 10 coin flips.\n\nThe number of successful sales out of 100 calls made.\n\nThe number of defective items in a sample of 50 items.",
      "metadata": {}
    },
    {
      "id": "12c9064a-d152-4115-ab2b-dabbaaf95808",
      "cell_type": "markdown",
      "source": "Question 4: Under what conditions is the binomial distribution used, and how does it relate to the Bernoulli\ndistribution?",
      "metadata": {}
    },
    {
      "id": "6176eb63-b3be-4d06-9531-fefa982f3571",
      "cell_type": "markdown",
      "source": "The binomial distribution is used under these conditions:\n\nFixed number of trials: You perform the experiment a set number of times, say n times.\n\nTwo possible outcomes: Each trial results in either \"success\" or \"failure.\"\n\nConstant probability: The probability of success p stays the same on every trial.\n\nIndependent trials: The outcome of one trial doesn‚Äôt affect another.\n\nThe binomial distribution answers questions like: \"If I flip a coin 10 times, what‚Äôs the probability of getting exactly 6 heads?\"\n\nHow does it relate to the Bernoulli distribution?\n\nA Bernoulli distribution is like the building block of the binomial distribution.\n\nA single Bernoulli trial models just one experiment with two outcomes (success with probability p, failure with probability 1‚àíp).\n\nA binomial random variable is the sum of n independent Bernoulli trials.",
      "metadata": {}
    },
    {
      "id": "e6a10e5e-dcb0-4be0-a97e-be2678c7bfc1",
      "cell_type": "code",
      "source": "Question5: What are the key properties of the Poisson distribution, and when is it appropriate to use this\ndistribution?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8fb464e7-ecd5-4b8d-8489-ba56a6aad770",
      "cell_type": "markdown",
      "source": "Key Properties of the Poisson Distribution:\nDiscrete distribution: It counts the number of events.\n\nDefined by a single parameter: Œª , which is the average rate of occurrence in a fixed interval (of time, space, etc.).\n\nEvents are independent: The occurrence of one event doesn't affect the probability of another.\n\nRare events: Typically models rare events happening over a fixed period or space.\n\nProbability mass function (PMF):P(X=k)= e ‚àíŒª Œª k/k!\n \nwhere \nk is the number of events (0, 1, 2, ...).\n\nMean and variance: Both are equal to Œª.\n\nWhen to Use the Poisson Distribution:\nUse it when:\n\nYou are counting the number of times an event happens.\n\nThe events happen independently.\n\nThe average rate \nŒª is constant.\n\nTwo events can't occur at exactly the same instant (the probability of simultaneous events is very small).\n\nYou are dealing with a fixed interval of time, area, volume, etc.\n\nExample Situations:\nNumber of emails you get in an hour \n\nNumber of cars passing through a tollbooth in 10 minutes \n\nNumber of typos on a printed page \n\nNumber of earthquakes in a region over a year ",
      "metadata": {}
    },
    {
      "id": "d238e557-0f38-4034-b5c7-4c71b9747e2a",
      "cell_type": "markdown",
      "source": "Question6: Define the terms \"probability distribution\" and \"probability density function\" (PDF). How does a\nPDF differ from a probability mass function (PMF)?",
      "metadata": {}
    },
    {
      "id": "3830fb69-23c5-4c28-9479-a1d5158bb465",
      "cell_type": "markdown",
      "source": "Probability Distribution\nA probability distribution shows how likely different outcomes are.\n\nIt tells us where the chances are placed across all possible results.\n\nExample: If you roll a dice, each number (1‚Äì6) has a probability of 1/6\n\n\nProbability Density Function (PDF)\nA PDF is used when the outcomes can be any value in a range (like 1.5, 2.7, etc.).\n\nIt shows how tightly packed or spread out the chances are along a line.\n\nThe height of the curve tells you how \"likely\" values are around that point.\n\nImportant: You don‚Äôt get a probability at one exact point ‚Äî you only get probabilities over a range.\n\nDifference between PDF and PMF\n\nPDF (Probability Density Function)                                 PMF (Probability Mass Function)\nUsed for continuous things (like height, weight)                   Used for discrete things (like rolling a dice)\nGives density, not direct probability at a point                   Gives actual probability at a point\nProbability of exact number = 0\t                                   Probability of exact number > 0\nFind probabilities by finding area under curve\t                   Find probabilities by adding up values",
      "metadata": {}
    },
    {
      "id": "5d470c78-31c9-486f-ba2b-74f583e0c9e1",
      "cell_type": "code",
      "source": "Question7: Explain the Central Limit Theorem (CLT) with example",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2f930063-7bac-4f57-941f-29d556b9dd51",
      "cell_type": "markdown",
      "source": "The Central Limit Theorem (CLT) is a fundamental concept in statistics that explains the emergence of the normal distribution in the study of sample means. It states that, regardless of the shape of the original population distribution ‚Äî whether it is skewed, uniform, or irregular ‚Äî the distribution of the sample means will approach a normal distribution as the sample size becomes large, typically \nn‚â•30. This remarkable result holds true even when the underlying population is not normally distributed. In essence, while individual data points may display high variability or asymmetry, the averages of sufficiently large random samples will cluster symmetrically around the true population mean, forming the characteristic bell-shaped curve of the normal distribution. For example, consider a survey measuring the number of chocolates consumed by individuals in a month. Although the original data may be highly skewed, with some people consuming very few chocolates and others consuming large amounts, repeatedly sampling groups of 30 individuals and calculating the average chocolate consumption would produce a distribution of sample means that is approximately normal.\n\nThe Central Limit Theorem is crucial because it provides the theoretical foundation for many statistical methods, including confidence intervals and hypothesis testing, even when the original data are not normally distributed.\n\nReal-world application: In a hospital setting, patient recovery times after surgery can vary greatly. Individual recovery times may be influenced by a range of factors such as age, health conditions, and type of surgery, leading to a highly skewed distribution. However, if the hospital collects numerous random samples of recovery times and calculates the mean recovery time for each sample, the distribution of these means will approximate a normal distribution. This enables the hospital to make reliable predictions about average recovery periods and better allocate medical resources, despite the unpredictability of individual cases.\n",
      "metadata": {}
    },
    {
      "id": "cc808c63-f216-43c8-8380-8172cb8dc158",
      "cell_type": "markdown",
      "source": "Question8: Compare z-scores and t-scores. When should you use a z-score, and when should a t-score be applied instead?",
      "metadata": {}
    },
    {
      "id": "ae1b04ff-fad4-4a36-ab29-7e89aa2ef594",
      "cell_type": "markdown",
      "source": "Comparison of Z-scores and T-scores\nZ-scores and T-scores are both used to describe how far a data point is from the mean, measured in terms of standard deviations. However, they are used in different situations depending on what information is known about the population and the sample size.\n\nZ-scores\nA z-score is used when the population standard deviation (œÉ) is known and the sample size is large (typically greater than 30). The z-score follows the standard normal distribution, which is a perfectly symmetrical bell curve. The formula for calculating a z-score is\n\nz = (X - Œº) / œÉ\n\nwhere:\n\nX = data point\n\nŒº = population mean\n\nœÉ = population standard deviation\n\nZ-scores assume less uncertainty because they are based on complete information about the population.\n\nT-scores\nA t-score is used when the population standard deviation is unknown and the sample size is small (typically less than 30). T-scores follow the t-distribution, which is similar to the normal distribution but has heavier tails to account for extra uncertainty in small samples. The formula for a t-score is\n\nt = (X - Œº) / (s / ‚àön)\n\nwhere:\n\nX = data point\n\nŒº = sample mean\n\ns = sample standard deviation\n\nn = sample size\n\nAs the sample size increases, the t-distribution becomes almost identical to the normal distribution.\n\nWhen to Use Each\nUse a Z-score when:\n\nThe population standard deviation (œÉ) is known.\n\nThe sample size is large (n > 30).\n\nUse a T-score when:\n\nThe population standard deviation is unknown.\n\nThe sample size is small (n < 30).\n",
      "metadata": {}
    },
    {
      "id": "0d30a3ac-d225-4215-aab8-898cbfe89801",
      "cell_type": "markdown",
      "source": "Question9: Given a sample mean of 105, a population mean of 100, a standard deviation of 15, and a sample\nsize of 25, calculate the z-score and p-value. Based on a significance level of 0.05, do you reject or fail to\nreject the null hypothesis?\n\n Task: Write Python code to calculate the z-score and p-value for the given data.\n\nObjective: Apply the formula for the z-score and interpret the p-value for hypothesis testing.",
      "metadata": {}
    },
    {
      "id": "5cc2c882-5fc7-4694-ac17-d0e20029486e",
      "cell_type": "code",
      "source": "import scipy.stats as stats\n\n# Given data\nsample_mean = 105\npopulation_mean = 100\nstd_dev = 15\nsample_size = 25\nalpha = 0.05  # Significance level\n\n# Calculate the standard error\nstandard_error = std_dev / (sample_size ** 0.5)\n\n# Calculate the z-score\nz_score = (sample_mean - population_mean) / standard_error\n\n# Calculate the p-value (two-tailed test)\np_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n\n# Decision\nif p_value < alpha:\n    decision = \"Reject the null hypothesis\"\nelse:\n    decision = \"Fail to reject the null hypothesis\"\n\n# Output the results\nprint(f\"Z-score: {z_score:.2f}\")\nprint(f\"P-value: {p_value:.4f}\")\nprint(f\"Decision: {decision}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Z-score: 1.67\nP-value: 0.0956\nDecision: Fail to reject the null hypothesis\n"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "6648ff94-1f2f-4a8a-9ab4-63f20cba31ce",
      "cell_type": "markdown",
      "source": "Question10: Simulate a binomial distribution with 10 trials and a probability of success of 0.6 using Python.\nGenerate 1,000 samples and plot the distribution. What is the expected mean and variance?\n\nTask: Use Python to generate the data, plot the distribution, and calculate the mean and variance.\n\nObjective: Understand the properties of a binomial distribution and verify them through simulation.",
      "metadata": {}
    },
    {
      "id": "33f431bd-00ce-443d-a1d0-48d552b0f73b",
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Parameters\nn_trials = 10         # number of trials\np_success = 0.6       # probability of success\nn_samples = 1000      # number of samples\n\n# Generate binomial samples\nsamples = np.random.binomial(n=n_trials, p=p_success, size=n_samples)\n\n# Plot the distribution\nplt.figure(figsize=(8, 5))\nsns.histplot(samples, bins=range(0, n_trials + 2), kde=False, color='skyblue', edgecolor='black')\nplt.title('Binomial Distribution (n=10, p=0.6)')\nplt.xlabel('Number of Successes')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n\n# Calculate mean and variance\nsample_mean = np.mean(samples)\nsample_variance = np.var(samples)\n\n# Theoretical mean and variance\ntheoretical_mean = n_trials * p_success\ntheoretical_variance = n_trials * p_success * (1 - p_success)\n\n# Output the results\nprint(f\"Sample Mean: {sample_mean:.2f}\")\nprint(f\"Sample Variance: {sample_variance:.2f}\")\nprint(f\"Theoretical Mean: {theoretical_mean}\")\nprint(f\"Theoretical Variance: {theoretical_variance}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Matplotlib is building the font cache; this may take a moment.\n"
        },
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'seaborn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Set random seed for reproducibility\u001b[39;00m\n\u001b[1;32m      6\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 3
    },
    {
      "id": "31462cda-101a-437c-a5ab-6aaf9f97e4f3",
      "cell_type": "code",
      "source": "!pip install seaborn\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'OSError'>",
          "evalue": "Not available",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip install seaborn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2653\u001b[0m, in \u001b[0;36mInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2648\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[1;32m   2651\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[1;32m   2652\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[0;32m-> 2653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/IPython/utils/_process_emscripten.py:11\u001b[0m, in \u001b[0;36msystem\u001b[0;34m(cmd)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msystem\u001b[39m(cmd):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mOSError\u001b[0m: Not available"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 4
    },
    {
      "id": "b8e47e51-382d-410f-a405-d8f040d6c9b2",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}