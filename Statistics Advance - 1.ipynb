{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "93e73bb5-6c56-43f9-bf36-c9baff0da7fe",
      "cell_type": "markdown",
      "source": "1. Explain the properties of the F-distribution. ",
      "metadata": {}
    },
    {
      "id": "6633ff39-b15f-45ea-aa89-823d3c7c4e32",
      "cell_type": "markdown",
      "source": "F-Distribution: Properties\nThe F-distribution is a probability distribution that arises when comparing two sample variances. It is most commonly used in ANOVA (Analysis of Variance) and in tests of whether two populations have the same variance.\n\nHere are the key properties:\n\nShape and Range:\n\nThe F-distribution is positively skewed (it has a long right tail).\n\nIt only takes positive values because variances are always positive.\n\nIt is defined for values between 0 and ∞.\n\nDegrees of Freedom:\n\nIt has two types of degrees of freedom:\n\ndf₁: Degrees of freedom for the numerator (related to the first sample).\n\ndf₂: Degrees of freedom for the denominator (related to the second sample).\n\nThe shape of the distribution depends on both df₁ and df₂.\n\nMean and Variance:\nMean = df₂ / (df₂ - 2)   (for df₂ > 2)\n\nVariance = [2 * (df₂)² * (df₁ + df₂ - 2)] / [df₁ * (df₂ - 2)² * (df₂ - 4)]   (for df₂ > 4)\n\nRelation to Variances:\n\nThe F-statistic is the ratio of two scaled variances.\n\nDependence on Sample Size:\n\nAs the sample sizes increase (degrees of freedom get larger), the F-distribution becomes less skewed and starts to look more symmetric.\n\nApplications:\n\nTesting equality of variances (F-test).\n\nANOVA: To test if multiple group means are equal by comparing group variances.\n\nRegression analysis: To test the overall significance of a model.",
      "metadata": {}
    },
    {
      "id": "b2213e76-7da8-49ff-85a5-f59afea80f7e",
      "cell_type": "markdown",
      "source": "2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?",
      "metadata": {}
    },
    {
      "id": "da132fca-ddbf-48aa-9558-31c581b7da62",
      "cell_type": "markdown",
      "source": "Uses of the F-Distribution in Statistical Tests\nThe F-distribution is mainly used in statistical tests that involve comparing two or more variances. It is appropriate because the F-distribution is based on the ratio of two independent sample variances, which is exactly what these tests require.\n\nHere are the main tests where the F-distribution is used:\n\nANOVA (Analysis of Variance):\n\nPurpose: To compare the means of three or more groups.\n\nWhy F-distribution?: ANOVA compares the variance between groups to the variance within groups. The test statistic follows an F-distribution because it is the ratio of two variances.\n\nF-Test for Equality of Variances:\n\nPurpose: To check if two populations have the same variance.\n\nWhy F-distribution?: The test uses the ratio of two sample variances. If the variances are equal, this ratio follows an F-distribution.\n\nRegression Analysis (Overall Significance Test):\n\nPurpose: To check if a linear regression model explains a significant amount of the variation in the dependent variable.\n\nWhy F-distribution?: The F-statistic tests if at least one regression coefficient is different from zero by comparing the model variance to the residual (error) variance.\n\nModel Comparison:\n\nPurpose: To compare two statistical models (for example, a full model vs. a reduced model).\n\nWhy F-distribution?: The difference in model fit can be evaluated by comparing their variances, again leading to an F-distribution.\n\nWhy is the F-distribution appropriate?\nIt models the ratio of two variances.\n\nVariances are always positive, and the F-distribution is only defined for positive values.\n\nIt accounts for different sample sizes using degrees of freedom in both the numerator and denominator.",
      "metadata": {}
    },
    {
      "id": "e0fb286c-2af8-4998-9ab7-40632ab883fb",
      "cell_type": "markdown",
      "source": "3. What are the key assumptions required for conducting an F-test to compare the variances of two\npopulations?",
      "metadata": {}
    },
    {
      "id": "9e92d946-adbf-4ec6-9966-8b48a79bd5bb",
      "cell_type": "markdown",
      "source": "Key Assumptions for Conducting an F-Test to Compare Two Population Variances\nTo correctly use an F-test for comparing the variances of two populations, the following assumptions must be satisfied:\n\nIndependence:\n\nThe two samples must be independent of each other.\n\nThat means the selection of one sample should not influence the selection of the other.\n\nNormality:\n\nBoth populations should be normally distributed.\n\nThe F-test is very sensitive to departures from normality, so if the data are not normal, the results may not be valid.\n\nRandom Sampling:\n\nThe data should come from random samples.\n\nThis ensures that the samples represent the populations properly.\n\nScale of Measurement:\n\nThe data should be measured on at least an interval scale (where differences between values are meaningful).\n\n",
      "metadata": {}
    },
    {
      "id": "d949a24f-5c67-4cde-a3a7-592a156925f1",
      "cell_type": "markdown",
      "source": "4. What is the purpose of ANOVA, and how does it differ from a t-test? ",
      "metadata": {}
    },
    {
      "id": "8db80817-5350-437c-ab79-dfdbf73e1042",
      "cell_type": "markdown",
      "source": "Purpose of ANOVA\nThe purpose of ANOVA (Analysis of Variance) is to determine whether there are statistically significant differences between the means of three or more independent groups.\n\nInstead of comparing groups two at a time (which would increase the chance of error), ANOVA tests all group means at once.\n\nIt looks at how much of the overall variability in the data is due to differences between groups compared to random variability within groups.\n\nHow ANOVA Differs from a t-test\nFeature                 \t                                 t-test                                         \tANOVA\nNumber of Groups                                             Compares two groups only\t                        Compares three or more groups\nPurpose                                                      Checks if the means of two groups are different\tChecks if at least one group mean is different\nTest Statistic\t                                             Uses the t-statistic\t                            Uses the F-statistic\nRisk of Error\t                                             Multiple t-tests increase Type I error         \tANOVA controls Type I error properly\nFollow-up Analysis\t                                         No immediate follow-up needed if significant       If significant, post-hoc tests are used to find which groups differ",
      "metadata": {}
    },
    {
      "id": "e78abf3f-55e2-4ecd-aff0-fe117fad3966",
      "cell_type": "markdown",
      "source": "5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\nthan two groups.",
      "metadata": {}
    },
    {
      "id": "750e5db6-fdfd-481a-9292-a634dd9d3da7",
      "cell_type": "markdown",
      "source": "When to Use One-Way ANOVA\nYou should use a one-way ANOVA when you want to compare the means of three or more independent groups based on one factor (one independent variable).\n\nWhy Use One-Way ANOVA Instead of Multiple t-tests\nControls Type I Error (False Positives):\n\nIf you perform multiple t-tests between groups, the chance of making a Type I error (incorrectly rejecting a true null hypothesis) increases with each test.\n\nANOVA controls the overall error rate, keeping the probability of a false positive low.\n\nMore Efficient and Organized:\n\nInstead of doing many separate t-tests (which gets messy and time-consuming), ANOVA gives a single test to check if any group means are different.\n\nProvides Overall Insight:\n\nANOVA tells you whether there is any difference at all between the groups.\n\nIf the result is significant, you can then do post-hoc tests (like Tukey’s test) to find out which groups are different.\n\nExample\nSuppose you want to compare the exam scores of students from three different classes (Class A, Class B, and Class C):\n\nMultiple t-tests:\n\nYou would need to compare Class A vs. B, Class A vs. C, and Class B vs. C → 3 separate t-tests.\n\nEach t-test increases the risk of Type I error.\n\nOne-way ANOVA:\n\nYou perform one single test to check if there is any difference among the three classes.\n\nIf the test is significant, you then explore further with post-hoc analysis.\n\n",
      "metadata": {}
    },
    {
      "id": "0871b1eb-787c-4c69-8bc7-1ccbe925b1ec",
      "cell_type": "markdown",
      "source": "6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\nHow does this partitioning contribute to the calculation of the F-statistic?",
      "metadata": {}
    },
    {
      "id": "ef1ee687-7855-4362-a742-134757e3dcf9",
      "cell_type": "markdown",
      "source": "In ANOVA, the total variance in the data is partitioned (split) into two parts:\n\nBetween-Group Variance (SSB):\n\nMeasures the variability between the group means and the overall mean.\n\nIt shows how much the groups differ from each other.\n\nLarge between-group variance suggests that the group means are different.\n\nWithin-Group Variance (SSW):\n\nMeasures the variability within each group (how much the individual scores in a group differ from their group mean).\n\nIt shows the random error or natural variation within each group.\n\nSmaller within-group variance means that individuals in the same group are more similar.\n\nHow This Partitioning Contributes to the F-Statistic\nAfter splitting the variance, ANOVA compares the between-group variance to the within-group variance using the F-statistic.\n\nThe F-statistic is calculated as:\n\n𝐹 =\nMean Square Between Groups (MSB)/\nMean Square Within Groups (MSW)\n\nWhere:\n\nMSB = \\frac{SSB}{\\text{df}_{\\text{between}}} ]\n\nMSW = \\frac{SSW}{\\text{df}_{\\text{within}}} ]\n\nExplanation:\n\nIf the between-group variance (MSB) is much larger than the within-group variance (MSW), the F-value will be large.\n\nA large F-value suggests that the group means are significantly different.\n\nIf the group means are similar, the F-value will be close to 1.\n\nQuick Example\nImagine you are comparing the test scores of students from three different schools:\n\nBetween-group variance: Differences in average scores between the schools.\n\nWithin-group variance: Differences among students within the same school.\n\nIf schools are very different, between-group variance will be large, leading to a large F-statistic and possibly a significant result.",
      "metadata": {}
    },
    {
      "id": "ec5367f7-40ec-419f-8869-d62db07bd48f",
      "cell_type": "markdown",
      "source": "7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\ndifferences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?",
      "metadata": {}
    },
    {
      "id": "f1f08bb1-9933-43af-879e-0a43e7793327",
      "cell_type": "markdown",
      "source": "Comparison of Classical (Frequentist) ANOVA vs Bayesian ANOVA\nAspect\t                                                    Classical (Frequentist) ANOVA\t                                                                            Bayesian ANOVA\nUncertainty                                                 Handled using p-values and confidence intervals.\t                                             Handled using probability  distributions (posteriors).\nParameter Estimation\t                                    Estimates fixed values (e.g., means, variances).\t                                             Estimates distributions of parameters (beliefs updated).\nHypothesis Testing\t                                        Reject or fail to reject the null hypothesis based on p-value.\t                                 Calculate probability that one model is better than another.\nInterpretation                                           \tResults are based on long-run frequencies of repeated sampling.\t                                 Results are interpreted as direct probabilities.\nPrior Information                                           Does not use prior information.\t                                                                 Can include prior knowledge about parameters.\nOutput\t                                                    F-statistic, p-value.\t                                                                         Posterior distributions, Bayes Factors, Credible Intervals.\n\nIn Simple Words\nFrequentist ANOVA:\n\nFocuses on whether the observed differences between groups could have happened by random chance.\n\nUses p-values: if p < 0.05, you reject the null hypothesis.\n\nBayesian ANOVA:\n\nFocuses on how likely different hypotheses are given the observed data.\n\nYou don't just \"reject\" or \"fail to reject\"; instead, you update your belief about the groups based on the data.\n\nYou can say things like \"there is a 95% chance this group mean is larger than the others.\"\n\n\n",
      "metadata": {}
    },
    {
      "id": "39bd4da9-f57e-441b-badc-b36429f6e28b",
      "cell_type": "markdown",
      "source": "8. Question: You have two sets of data representing the incomes of two different professions1\nV Profession A: [48, 52, 55, 60, 62'\nV Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\nincomes are equal. What are your conclusions based on the F-test?\n\nTask: Use Python to calculate the F-statistic and p-value for the given data.\n\nObjective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.",
      "metadata": {}
    },
    {
      "id": "0c9af2ba-ee86-4937-b1e9-c6d327de7fc1",
      "cell_type": "code",
      "source": "import scipy.stats as stats\nimport numpy as np\n\n# Given data\nprofession_a = [48, 52, 55, 60, 62]\nprofession_b = [45, 50, 55, 52, 47]\n\n# Calculate sample variances\nvar_a = np.var(profession_a, ddof=1)\nvar_b = np.var(profession_b, ddof=1)\n\n# Calculate F-statistic\nF_statistic = var_a / var_b\n\n# Degrees of freedom\ndf1 = len(profession_a) - 1\ndf2 = len(profession_b) - 1\n\n# Calculate p-value\np_value = 1 - stats.f.cdf(F_statistic, df1, df2)\n\nprint(f\"Variance of Profession A: {var_a:.2f}\")\nprint(f\"Variance of Profession B: {var_b:.2f}\")\nprint(f\"F-statistic: {F_statistic:.2f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\n# Interpret the result\nalpha = 0.05\nif p_value < alpha:\n    print(\"Reject the null hypothesis: Variances are different.\")\nelse:\n    print(\"Fail to reject the null hypothesis: Variances are equal.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Variance of Profession A: 32.80\nVariance of Profession B: 15.70\nF-statistic: 2.09\nP-value: 0.2465\nFail to reject the null hypothesis: Variances are equal.\n"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "829186db-fbaa-46a9-b88f-799d4eb88d7e",
      "cell_type": "markdown",
      "source": "9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\naverage heights between three different regions with the following data1\nV Region A: [160, 162, 165, 158, 164'\nV Region B: [172, 175, 170, 168, 174'\nV Region C: [180, 182, 179, 185, 183'\nV Task: Write Python code to perform the one-way ANOVA and interpret the results\f\nV Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.",
      "metadata": {}
    },
    {
      "id": "79b09e91-81fb-419a-9a2d-c271672cf1f9",
      "cell_type": "code",
      "source": "# Given data\nregion_a = [160, 162, 165, 158, 164]\nregion_b = [172, 175, 170, 168, 174]\nregion_c = [180, 182, 179, 185, 183]\n\n# Perform one-way ANOVA\nf_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n\nprint(f\"F-statistic: {f_statistic:.2f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\n# Interpret the result\nalpha = 0.05\nif p_value < alpha:\n    print(\"Reject the null hypothesis: There is a significant difference in average heights between regions.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant difference in average heights between regions.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "F-statistic: 67.87\nP-value: 0.0000\nReject the null hypothesis: There is a significant difference in average heights between regions.\n"
        }
      ],
      "execution_count": 3
    },
    {
      "id": "1158d94e-8912-447b-98b5-9a7c12a9c274",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}